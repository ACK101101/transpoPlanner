{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import os.path as osp\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trans_infra.trans_infra.simulator import TransInfraNetworkModel\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(graph_file) -> nx.Graph:\n",
    "    \"\"\"loads in modified OSM graph\"\"\"\n",
    "    G_trans = ox.load_graphml(\n",
    "                graph_file,\n",
    "                node_dtypes={'index':int, 'x':float, 'y':float, 'general0':float, 'general1':float, \n",
    "                            'general2':float, 'general3':float, 'general4':float},    \n",
    "                edge_dtypes={'u':int, 'v':int, 'speed':float, 'capacity':float, 'general0':float,\n",
    "                            'general1':float, 'general2':float, 'general3':float})\n",
    "    G_trans = G_trans.to_undirected()                                   # make undirected\n",
    "    return G_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * set num GNN layers to be slightly larger than necessary receptive field for problem\n",
    "# * add MLP at end of GNN layers for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConditionedConvolution(nn.Module):\n",
    "    \"\"\"GNN block: updates node and edge embeddings\"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim, out_dim, dropout_p=0.2):\n",
    "        super(EdgeConditionedConvolution, self).__init__()\n",
    "        self.node_lin = nn.Linear(node_dim, hidden_dim)\n",
    "        self.edge_lin = nn.Linear(edge_dim, hidden_dim)\n",
    "        self.conv = gnn.NNConv(hidden_dim, out_dim, \n",
    "                               nn=nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                                nn.BatchNorm1d(hidden_dim),\n",
    "                                                nn.Dropout(p=dropout_p),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(hidden_dim, hidden_dim*out_dim),\n",
    "                                                nn.BatchNorm1d(hidden_dim*out_dim),\n",
    "                                                nn.Dropout(p=dropout_p),\n",
    "                                                nn.ReLU(),\n",
    "                                                ))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch) -> torch.Tensor:\n",
    "        x = self.node_lin(x)\n",
    "        edge_attr = self.edge_lin(edge_attr)\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "class EdgeRegressionModel(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim, out_dim):\n",
    "        super(EdgeRegressionModel, self).__init__()\n",
    "        self.conv1 = EdgeConditionedConvolution(node_dim, edge_dim, hidden_dim, hidden_dim)\n",
    "        self.conv2 = EdgeConditionedConvolution(hidden_dim, edge_dim, hidden_dim, hidden_dim)\n",
    "        self.conv3 = EdgeConditionedConvolution(hidden_dim, edge_dim, hidden_dim, hidden_dim)\n",
    "        self.regressor = nn.Linear(2*hidden_dim, out_dim)\n",
    "        self.pool = gnn.global_mean_pool\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.conv1(x, edge_index, edge_attr, batch)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr, batch)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr, batch)\n",
    "        # edge predictions\n",
    "        x_i = x[edge_index[0]]\n",
    "        x_j = x[edge_index[1]]\n",
    "        x_edge = torch.cat([x_i, x_j], dim=1)\n",
    "        edge_scores = self.regressor(x_edge)\n",
    "        # graph predictions\n",
    "        graph_embed = self.pool(x, batch)\n",
    "        \n",
    "        return edge_scores, graph_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"f(s) -> q : takes concat graph embeddings of states and predicts their q-vals\"\"\"\n",
    "    def __init__(self, graph_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(graph_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    # ? Add episode_length and budget as extra hyperparameters?\n",
    "    # ? precalculate betweenness?\n",
    "    \"\"\"planning agent\"\"\"\n",
    "    def __init__(self, node_dim, edge_dim, erm_hidden_dim,\n",
    "                 graph_dim, hidden_dim, num_actions, learning_rate, gamma, \n",
    "                 epsilon, epsilon_decay, epsilon_min, batch_size, memory_size,\n",
    "                 pop_size=300, episode_len=24):\n",
    "        # init GNN, Q NN, T NN, opt, loss, and memory buffer\n",
    "        self.graph_dim = graph_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.edge_model = EdgeRegressionModel(node_dim, edge_dim, \n",
    "                                              erm_hidden_dim, self.num_actions)\n",
    "        self.q_net = DQN(graph_dim, hidden_dim)\n",
    "        self.t_net = DQN(graph_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(list(self.edge_model.parameters()) + list(self.q_net.parameters()), \n",
    "                                    lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        # learning hyperparams\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        # init sim\n",
    "        self.idx2gen_edges = {0: 'car', 1: 'bus', 2: 'pedbike', 3: 'other'}         # TODO: make arg\n",
    "        self.pop_size = pop_size\n",
    "        self.episode_len = episode_len\n",
    "        self.path = \"./osm_dataset/raw/copenhagen.osm\"\n",
    "        self.load_sim()\n",
    "    \n",
    "    def load_sim(self):\n",
    "        self.sim = TransInfraNetworkModel(self.pop_size, self.episode_len, self.path)\n",
    "        self.data2nx = dict(zip(range(self.sim.G_trans.number_of_nodes()),\n",
    "                                self.sim.G_trans.nodes()))\n",
    "         \n",
    "    def sample_env(self, data, isNew=False):\n",
    "        \"\"\"get (s, a, r, s_+1) observation and save to replay\"\"\"\n",
    "        print(f\"isNew: {isNew}\")\n",
    "        # refresh sim if new\n",
    "        if isNew:\n",
    "            print(\"new env\")\n",
    "            self.path = data.path[0]\n",
    "            self.load_sim()\n",
    "        # run sim and get reward in curr state\n",
    "        print(\"running sim\")\n",
    "        for _ in range(self.episode_len):\n",
    "            self.sim.step()\n",
    "        curr_score = self.sim.datacollector.get_model_vars_dataframe().iloc[-1][0]\n",
    "        \n",
    "        # get state from graph_embedding\n",
    "        edge_scores, curr_state = self.edge_model(data.x, data.edge_index, \n",
    "                                                  data.edge_attr, data.batch)\n",
    "        \n",
    "        # get actions from edge_scores\n",
    "        edge_idxs, action_idxs = self.actions_from_edge_scores(edge_scores)\n",
    "        \n",
    "        # modify edge attrs by actions\n",
    "        graph_embeds = None\n",
    "        new_data_list = []\n",
    "        for i in range(len(action_idxs)):\n",
    "            new_data = self.take_actions(data, edge_idxs[i], action_idxs[i])\n",
    "            new_data_list.append(new_data)\n",
    "            # get graph embeddings of modified graphs\n",
    "            _, graph_embed = self.edge_model(new_data.x, new_data.edge_index, \n",
    "                                             new_data.edge_attr, new_data.batch)\n",
    "            if graph_embeds is None:    \n",
    "                graph_embeds = graph_embed\n",
    "            else:                     \n",
    "                graph_embeds = torch.cat((graph_embeds, graph_embed), dim=0)\n",
    "        \n",
    "        # pass concat graph embeds to Q NN and get Q vals\n",
    "        q_vals = self.q_net(graph_embeds)\n",
    "        # choose actions corresponding with max Q val\n",
    "        max_q_idx = torch.max(q_vals, dim=0).indices\n",
    "        max_edge_idx = edge_idxs[max_q_idx]\n",
    "        max_q_action_vec = edge_scores[max_edge_idx]\n",
    "        max_q_action_idx = torch.argmax(max_q_action_vec).item()\n",
    "        next_state = new_data_list[max_q_idx]\n",
    "        \n",
    "        # modify networkx graph\n",
    "        u, v = data.edge_index[0][max_edge_idx].item(), data.edge_index[1][max_edge_idx].item()\n",
    "        u_p, v_p = self.data2nx[u], self.data2nx[v]\n",
    "        \n",
    "        self.sim.G_trans.edges[u_p, v_p, 0][\"general\"] = self.idx2gen_edges[max_q_action_idx]\n",
    "        self.sim.reset()\n",
    "        print(\"reset and running sim again\")\n",
    "        # run sim and get reward in curr state\n",
    "        for _ in range(self.episode_len):\n",
    "            self.sim.step()\n",
    "        next_score = self.sim.datacollector.get_model_vars_dataframe().iloc[-1][0]\n",
    "        # print(f\"next_score: {next_score}\")\n",
    "        reward = next_score - curr_score\n",
    "        # print(f\"reward: {reward}\")\n",
    "        # ! need to pass action in a smarter way, incorporate the index of the edge?\n",
    "        # ? concat edge embedding and action\n",
    "        self.store_transition(data, max_q_action_idx, reward, next_state, False)     # ? action saved is idx\n",
    "\n",
    "    def actions_from_edge_scores(self, edge_scores) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"return best actions given edge_scores\"\"\"\n",
    "        # if explore randomly choose k edges\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            edge_idxs = np.random.choice(edge_scores.shape[0], \n",
    "                                       self.num_actions, replace=False)\n",
    "            action_vecs = edge_scores[edge_idxs]\n",
    "            actions = torch.max(action_vecs, 1)\n",
    "            return edge_idxs, actions.indices\n",
    "        # if exploit choose k edges with highest max element\n",
    "        max_elements = torch.max(edge_scores, axis=1)\n",
    "        edge_idxs = torch.argsort(max_elements.values)[-self.num_actions:]\n",
    "        action_vecs = edge_scores[edge_idxs]\n",
    "        actions = torch.max(action_vecs, 1)\n",
    "        \n",
    "        return edge_idxs, actions.indices\n",
    "\n",
    "    def take_actions(self, data, edge_idx, action_idx):\n",
    "        \"\"\"modify data object to reflect action\"\"\"\n",
    "        # TODO: make indexing not hardcoded\n",
    "        u, v = data.edge_index[0][edge_idx].item(), data.edge_index[1][edge_idx].item()\n",
    "        i = -1\n",
    "        for i in range(data.edge_index.shape[1]):\n",
    "            u2, v2 = data.edge_index[0][i].item(), data.edge_index[1][i].item()\n",
    "            if u == v2 and v == u2:\n",
    "                break\n",
    "\n",
    "        sym_edge_index = i\n",
    "\n",
    "        new_data = copy.deepcopy(data)\n",
    "        new_data.edge_attr[edge_idx, 3:7] = 0\n",
    "        new_data.edge_attr[edge_idx][action_idx] = 1\n",
    "        new_data.edge_attr[sym_edge_index, 3:7] = 0\n",
    "        new_data.edge_attr[sym_edge_index][action_idx] = 1\n",
    "        \n",
    "        return new_data\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.t_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def get_state_embedding(self, graph) -> torch.Tensor:\n",
    "        x, edge_index, edge_attr, batch = graph.x, graph.edge_index, graph.edge_attr, graph.batch\n",
    "        _, graph_emb = self.edge_model(x, edge_index, edge_attr, batch)\n",
    "        return graph_emb\n",
    "\n",
    "    def train(self, data):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            isNew = False if data.path[0] == self.path else True\n",
    "            self.sample_env(data, isNew)\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        state_data = Batch.from_data_list(states)\n",
    "        next_state_data = Batch.from_data_list(next_states)\n",
    "        # state_embeds = torch.squeeze(torch.stack(states, 1))\n",
    "        # next_state_embeds = torch.squeeze(torch.stack(next_states, 1))\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.Tensor(rewards).unsqueeze(1)\n",
    "        dones = torch.Tensor(dones).unsqueeze(1)\n",
    "        \n",
    "        state_edge_scores, state_graph_embeds = self.edge_model(state_data.x, state_data.edge_index, \n",
    "                                                                state_data.edge_attr, state_data.batch)\n",
    "        next_state_edge_scores, next_state_graph_embeds = self.edge_model(next_state_data.x, next_state_data.edge_index, \n",
    "                                                                          next_state_data.edge_attr, next_state_data.batch)\n",
    "\n",
    "        q_values = self.q_net(state_graph_embeds)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.t_net(next_state_graph_embeds)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./osm_dataset/raw/east.osm\n",
      "./osm_dataset/raw/copenhagen.osm\n",
      "./osm_dataset/raw/nairobi.osm\n",
      "./osm_dataset/raw/melbourne.osm\n",
      "./osm_dataset/raw/durham.osm\n",
      "./osm_dataset/raw/calgary.osm\n",
      "./osm_dataset/raw/la.osm\n",
      "./osm_dataset/raw/tehran.osm\n",
      "./osm_dataset/raw/hanoi.osm\n",
      "./osm_dataset/raw/seattle.osm\n",
      "./osm_dataset/raw/west.osm\n",
      "./osm_dataset/raw/kobe.osm\n",
      "./osm_dataset/raw/bradenton.osm\n",
      "./osm_dataset/raw/delft.osm\n",
      "./osm_dataset/raw/taipei.osm\n",
      "./osm_dataset/raw/vienna.osm\n",
      "./osm_dataset/raw/tunis.osm\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for osm_path in glob.glob('./osm_dataset/raw/*'):\n",
    "    print(osm_path)\n",
    "    osm_graph = load_network(osm_path)\n",
    "    data = from_networkx(osm_graph, \n",
    "                        group_node_attrs=[\"index\", \"general0\", \"general1\", \"general2\",\n",
    "                                        \"general3\",  \"general4\", \"x\", \"y\"], \n",
    "                        group_edge_attrs=[\"u\", \"v\", \"osmid\", \"general0\", \"general1\", \n",
    "                                        \"general2\", \"general3\", \"length\", \"speed\"])\n",
    "    data.path = osm_path\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|N| = 1148, |E| = 2890\n",
      "|F_n| = 8, |F_e| = 9\n"
     ]
    }
   ],
   "source": [
    "data = data_list[0]\n",
    "num_nodes, num_node_features = data.x.shape\n",
    "num_edges, num_edge_features = data.edge_attr.shape\n",
    "hidden_dim = 32\n",
    "out_dim = 4\n",
    "\n",
    "print(f\"|N| = {num_nodes}, |E| = {num_edges}\")\n",
    "print(f\"|F_n| = {num_node_features}, |F_e| = {num_edge_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planner for all instances of osm\n",
    "loader = DataLoader(data_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderkumar/Desktop/transpoPlanner/transpoPlanner/trans_infra/trans_infra/simulator.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  scale_dist_cost = (np.array(self.dist_costs) / max(self.dist_costs)) * 8\n"
     ]
    }
   ],
   "source": [
    "planner = DQNAgent(num_node_features, num_edge_features, \n",
    "                    erm_hidden_dim=32,\n",
    "                    graph_dim=32, hidden_dim=32, num_actions=3, \n",
    "                    learning_rate=1e-4, gamma=0.99, \n",
    "                    epsilon=0.9, epsilon_decay=1000, epsilon_min=0.05, \n",
    "                    batch_size=5, memory_size=100,\n",
    "                    pop_size=100, episode_len=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid = {\"./osm_dataset/raw/copenhagen.osm\",\n",
    "         \"./osm_dataset/raw/seattle.osm\",\n",
    "         \"./osm_dataset/raw/hanoi.osm\",\n",
    "         \"./osm_dataset/raw/tehran.osm\",\n",
    "         \"./osm_dataset/raw/durham.osm\",\n",
    "         \"./osm_dataset/raw/la.osm\"             # ?\n",
    "         }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./osm_dataset/raw/east.osm']\n",
      "isNew: True\n",
      "new env\n",
      "running sim\n",
      "reset and running sim again\n",
      "isNew: False\n",
      "running sim\n",
      "reset and running sim again\n",
      "isNew: False\n",
      "running sim\n",
      "reset and running sim again\n",
      "isNew: False\n",
      "running sim\n",
      "reset and running sim again\n",
      "isNew: False\n",
      "running sim\n",
      "reset and running sim again\n",
      "['./osm_dataset/raw/copenhagen.osm']\n",
      "['./osm_dataset/raw/nairobi.osm']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mplanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 181\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    177\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(dones)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    179\u001b[0m state_edge_scores, state_graph_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model(state_data\u001b[38;5;241m.\u001b[39mx, state_data\u001b[38;5;241m.\u001b[39medge_index, \n\u001b[1;32m    180\u001b[0m                                                         state_data\u001b[38;5;241m.\u001b[39medge_attr, state_data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m--> 181\u001b[0m next_state_edge_scores, next_state_graph_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43mnext_state_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net(state_graph_embeds)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 34\u001b[0m, in \u001b[0;36mEdgeRegressionModel.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr, batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index, edge_attr, batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 21\u001b[0m, in \u001b[0;36mEdgeConditionedConvolution.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_lin(x)\n\u001b[1;32m     20\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_lin(edge_attr)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:108\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m/var/folders/9c/5hbdnj_57xv861zqcz43bs6w0000gn/T/torch_geometric.nn.conv.nn_conv_NNConv_propagate_8it775b1.py:167\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    158\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    159\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    160\u001b[0m                 edge_attr\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch_geometric/nn/conv/nn_conv.py:120\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, edge_attr: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels_l, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(x_j\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), weight)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.9/site-packages/torch/fx/traceback.py:62\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_fn_seq_nr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_grad_fn_seq_nr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     59\u001b[0m         current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_grad_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_in_grad_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_stack\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_preserve_node_meta:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# train loop:\n",
    "# TODO: save replay buffer to disc\n",
    "for epoch in range(2):\n",
    "    for batch in loader:\n",
    "        print(batch.path)\n",
    "        for i in range(7):\n",
    "            planner.train(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "update_frequency = 10\n",
    "target_update_frequency = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Generate training data\n",
    "        for _ in range(update_frequency):\n",
    "            # Select an action using the current Q-network and an exploration strategy (e.g., epsilon-greedy)\n",
    "            action = select_action(q_net, state)\n",
    "            \n",
    "            # Execute the action in the environment\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store the transition in the replay memory\n",
    "            replay_memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train the Q-network\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            # Sample a batch of transitions from the replay memory\n",
    "            transitions = random.sample(replay_memory, batch_size)\n",
    "            \n",
    "            # Extract the states, actions, rewards, next states, and dones from the transitions\n",
    "            states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "            \n",
    "            # Convert the data to PyTorch tensors\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            # Compute the predicted Q-values and the target Q-values\n",
    "            predicted_q_values = q_net(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_q_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "            \n",
    "            # Compute the loss and perform optimization\n",
    "            loss = criterion(predicted_q_values, target_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update the target Q-network\n",
    "        if episode % target_update_frequency == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            x, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "            edge_scores, graph_emb = model(x, edge_index, edge_attr, batch.batch)\n",
    "            print(f\"graph emb shape: {graph_emb.shape}\")\n",
    "            \n",
    "            # Compute loss\n",
    "            #! delete\n",
    "            dummy_y = torch.from_numpy(np.random.rand(edge_scores.shape[0], edge_scores.shape[1]).astype('float32')).to(device)\n",
    "            loss = criterion(edge_scores, dummy_y)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss /= len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the EdgeRegressionModel\n",
    "edge_model = EdgeRegressionModel(num_node_features, num_edge_features,\n",
    "                                 hidden_dim, out_dim).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Example loss function (mean squared error)\n",
    "optimizer = optim.Adam(edge_model.parameters(), lr=0.001)  # Example optimizer (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph emb shape: torch.Size([4, 32])\n",
      "graph emb shape: torch.Size([4, 32])\n",
      "graph emb shape: torch.Size([3, 32])\n",
      "Epoch [1/100], Loss: 29622866892117.3320\n",
      "graph emb shape: torch.Size([4, 32])\n",
      "graph emb shape: torch.Size([4, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[307], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[306], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph emb shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#! delete\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dummy_y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(edge_scores, dummy_y)\n\u001b[1;32m     18\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 100  # Number of training epochs\n",
    "train(edge_model, loader, criterion, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass network through GAT A\n",
    "\n",
    "# threshold attention weights from GAT A \n",
    "# to get K most important edges to modify\n",
    "\n",
    "# iterate over K edges, passing the modified\n",
    "# graph through GAT B to get a graph embedding\n",
    "\n",
    "# feed embedding to DQN, predict Q value\n",
    "\n",
    "# choose action with highest Q value\n",
    "\n",
    "# simulate action and return score\n",
    "    # save (s_t, a, r, s_t+1) as training data\n",
    "\n",
    "# backprop on GAT A, GAT B, DQN\n",
    "\n",
    "# ? GAT A and B can be same?\n",
    "# ? feed hyperparam of episode length into networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
